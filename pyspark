#########################################################################
spark-submit --master local s.py
##########################################################################

from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
inputfile = "file:///home/dileepdominic/inputfile.txt"
tempoutput = sc.textFile(inputfile).cache()
num = tempoutput.filter(lambda s: 'pyspark' in s).count()
print(num)

##########################################################################

# Read the first line of file which contains a word 

lines = sc.textFile("file:///")
pythonlines = lines.filter(lambda s : "python" in s)
pythonlines.first()

##########################################################################
# To distribute values 
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x).collect()
for num in squared:
	print "%i " % (num)

##########################################################################3
